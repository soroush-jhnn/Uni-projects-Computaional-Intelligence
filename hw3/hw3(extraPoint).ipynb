{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_LTBtfMBvW_"
      },
      "source": [
        "# CNN\n",
        "\n",
        "### Instructions\n",
        "- Make a copy of this notebook in your own Colab and complete the questions there.\n",
        "- You can add more cells if necessary. You may also add descriptions to your code, though it is not mandatory.\n",
        "- Make sure the notebook can run through by *Runtime -> Run all*. **Keep all cell outputs** for grading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33dD1e8tii2"
      },
      "source": [
        "Install PyTorch and Skorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d42EwpKsAe1D"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJB3VQYDCUmh",
        "outputId": "7e63405e-c74c-4179-c808-b768ef6d6f9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 29.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20 kB 30.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 51 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 61 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 71 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 81 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 92 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 102 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 112 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 122 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 133 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 143 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 153 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 163 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 174 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 184 kB 15.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 193 kB 15.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q torch skorch torchvision torchtext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "3l_Dl6qxCXmv",
        "outputId": "766985b8-dd29-470f-a147-676d30a09fea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import skorch\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uevQtU7NtZ_-"
      },
      "source": [
        "## 1. Tensor Operations (20 points)\n",
        "\n",
        "Tensor operations are important in deep learning models. In this part, you are required to implement some common tensor operations in PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DeQOItkeQCx"
      },
      "source": [
        "### 1) Tensor squeezing, unsqueezing and viewing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAOmBE5ODwpP"
      },
      "source": [
        "Tensor squeezing, unsqueezing and viewing are important methods to change the dimension of a Tensor, and the corresponding functions are [torch.squeeze](https://pytorch.org/docs/stable/torch.html#torch.squeeze), [torch.unsqueeze](https://pytorch.org/docs/stable/torch.html#torch.unsqueeze) and [torch.Tensor.view](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hVrM80YxFSjb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 1, 2, 1])\n",
            "torch.Size([3, 2])\n",
            "torch.Size([6])\n"
          ]
        }
      ],
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Add two new dimensions to x by using the function torch.unsqueeze, so that the size of x becomes (3, 1, 2, 1).\n",
        "x = torch.unsqueeze(x, 1)\n",
        "x = torch.unsqueeze(x, 3)\n",
        "print(x.shape)\n",
        "# Remove the two dimensions justed added by using the function torch.squeeze, and change the size of x back to (3, 2).\n",
        "x = torch.squeeze(x)\n",
        "print(x.shape)\n",
        "# x is now a two-dimensional tensor, or in other words a matrix. Now use the function torch.Tensor.view and change x to a one-dimensional vector with size being (6).\n",
        "x= x.view(6)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liuR-U0wea0n"
      },
      "source": [
        "### 2) Tensor concatenation and stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkbnt6v8Bo-j"
      },
      "source": [
        "Tensor concatenation and stack are operations to combine small tensors into big tensors. The corresponding functions are [torch.cat](https://pytorch.org/docs/stable/torch.html#torch.cat) and [torch.stack](https://pytorch.org/docs/stable/torch.html#torch.stack). Please read the documents of the functions, and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b9KqXu3Stfjh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3, 2])\n",
            "torch.Size([2, 3, 2])\n"
          ]
        }
      ],
      "source": [
        "# x is a tensor with size being (3, 2)\n",
        "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# y is a tensor with size being (3, 2)\n",
        "y = torch.Tensor([[-1, -2], [-3, -4], [-5, -6]])\n",
        "\n",
        "# Our goal is to generate a tensor z with size as (2, 3, 2), and z[0,:,:] = x, z[1,:,:] = y.\n",
        "# Use torch.stack to generate such a z\n",
        "z = torch.stack((x, y), 0) \n",
        "print(z.shape)\n",
        "# Use torch.cat and torch.unsqueeze to generate such a z\n",
        "z = torch.cat((torch.unsqueeze(x,0), torch.unsqueeze(y,0)), 0) \n",
        "print(z.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGw4eEo-eeHm"
      },
      "source": [
        "### 3) Tensor expansion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAII9eJgJFK2"
      },
      "source": [
        "Tensor expansion is to expand a tensor into a larger tensor along singleton dimensions. The corresponding functions are [torch.Tensor.expand](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand) and [torch.Tensor.expand_as](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.expand_as). Please read the documents of the functions, and finish the following practice. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sQbFte-AJzVL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3])\n",
            "torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "# x is a tensor with size being (3)\n",
        "x = torch.Tensor([1, 2, 3])\n",
        "\n",
        "# Our goal is to generate a tensor z with size (2, 3), so that z[0,:,:] = x, z[1,:,:] = x.\n",
        "\n",
        "# [TO DO]\n",
        "# Change the size of x into (1, 3) by using torch.unsqueeze.\n",
        "x = x.unsqueeze(0)\n",
        "print(x.shape)\n",
        "# [TO DO]\n",
        "# Then expand the new tensor to the target tensor by using torch.Tensor.expand.\n",
        "x = x.expand((2, 3))\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rFL_Shoef3m"
      },
      "source": [
        "### 4) Tensor reduction in a given dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEoJVw0LL9H"
      },
      "source": [
        "In deep learning, we often need to compute the mean/sum/max/min value in a given dimension of a tensor. Please read the document of [torch.mean](https://pytorch.org/docs/stable/torch.html#torch.mean), [torch.sum](https://pytorch.org/docs/stable/torch.html#torch.sum), [torch.max](https://pytorch.org/docs/stable/torch.html#torch.max), [torch.min](https://pytorch.org/docs/stable/torch.html#torch.min), [torch.topk](https://pytorch.org/docs/stable/torch.html#torch.topk), and finish the following practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "A7dlZwe4MNxo"
      },
      "outputs": [],
      "source": [
        "# x is a random tensor with size being (10, 50)\n",
        "x = torch.randn(10, 50)\n",
        "\n",
        "# Compute the mean value for each row of x.\n",
        "# You need to generate a tensor x_mean of size (10), and x_mean[k, :] is the mean value of the k-th row of x.\n",
        "x_mean = torch.mean(x, 1)\n",
        "# Compute the sum value for each row of x.\n",
        "# You need to generate a tensor x_sum of size (10).\n",
        "x_sum = torch.sum(x, 1)\n",
        "# Compute the max value for each row of x.\n",
        "# You need to generate a tensor x_max of size (10).\n",
        "x_max = torch.max(x, 1)[0] #because torch.max returns tensors (max, max_indices)\n",
        "# Compute the min value for each row of x.\n",
        "# You need to generate a tensor x_min of size (10).\n",
        "x_min = torch.min(x, 1)[0] #because torch.min returns tensors (min, min_indices)\n",
        "# Compute the top-5 values for each row of x.\n",
        "# You need to generate a tensor x_mean of size (10, 5), and x_top[k, :] is the top-5 values of each row in x.\n",
        "x_top = torch.topk(x, 5)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I49qjiqHB9oa"
      },
      "source": [
        "## Convolutional Neural Networks (40 points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JePbG5pSt1xv"
      },
      "source": [
        "Implement a convolutional neural network for image classification on CIFAR-10 dataset.\n",
        "\n",
        "CIFAR-10 is an image dataset of 10 categories. Each image has a size of 32x32 pixels. The following code will download the dataset, and split it into `train` and `test`. For this question, we use the default validation split generated by Skorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sQxOUQ29BuMB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to classifier_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to classifier_data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 2424832/9912422 [02:40<39:46, 3137.78it/s] "
          ]
        }
      ],
      "source": [
        "from torchvision import datasets,transforms \n",
        "from torch.utils.data import Dataset, DataLoader,TensorDataset,random_split,SubsetRandomSampler\n",
        "\n",
        "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST('classifier_data', train=True, download=True,transform=transform)\n",
        "test_dataset  = torchvision.datasets.MNIST('classifier_data', train=False, download=True,transform=transform)\n",
        "\n",
        "m=len(train_dataset)\n",
        "\n",
        "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
        "\n",
        "y_train = np.array([y for x, y in iter(train_dataset)])\n",
        "y_test = np.array([y for x, y in iter(test_dataset)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieBpiwMwi6wD"
      },
      "source": [
        "The following code visualizes some samples in the dataset. You may use it to debug your model if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cU5HrxybupyJ"
      },
      "outputs": [],
      "source": [
        "def plot(data, labels=None, num_sample=5):\n",
        "  n = min(len(data), num_sample)\n",
        "  for i in range(n):\n",
        "    plt.subplot(1, n, i+1)\n",
        "    plt.imshow(data[i], cmap=\"gray\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    if labels is not None:\n",
        "      plt.title(labels[i])\n",
        "\n",
        "train.labels = [train.classes[target] for target in train.targets]\n",
        "plot(train.data, train.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwzKmdcuCv1D"
      },
      "source": [
        "### 1) Basic CNN implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbEYo5WgjTtm"
      },
      "source": [
        "Consider a basic CNN model\n",
        "\n",
        "- It has 3 convolutional layers, followed by a linear layer.\n",
        "- Each convolutional layer has a kernel size of 3, a padding of 1.\n",
        "- ReLU activation is applied on every hidden layer.\n",
        "\n",
        "Please implement this model in the following section. You will need to tune the hyperparameters and fill the results in the table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZKyE2GUfL-Z"
      },
      "source": [
        "#### a) Implement convolutional layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P_aYytExtq9"
      },
      "source": [
        "Implement the initialization function and the forward function of the CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDmCKUD1LBFk"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
        "            nn.Linear(128 , 128),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Linear(128 , 128),            \n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Linear(128 , 128),\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(128*28*28 , 1024),\n",
        "            nn.Linear(1024, 10))\n",
        "  def forward(self, images):\n",
        "    return self.network(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_YaASPpgRiL"
      },
      "source": [
        "#### b) Tune hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygMcDdpy6XWP"
      },
      "source": [
        "Train the CNN model on CIFAR-10 dataset. Tune the number of channels, optimizer, learning rate and the number of epochs for best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PHxOHYnr8TU"
      },
      "outputs": [],
      "source": [
        "# implement hyperparameters here\n",
        "model = skorch.NeuralNetClassifier(CNN, criterion=torch.nn.CrossEntropyLoss,\n",
        "                                   device=\"cuda\")\n",
        "# implement input normalization & type cast here\n",
        "model.fit(train.data, train.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-EHKzozkRbD"
      },
      "source": [
        "Write down **validation accuracy** of your model under different hyperparameter settings. Note the validation set is automatically split by Skorch during `model.fit()`.\n",
        "\n",
        "**Hint:** You may need more epochs for SGD than Adam.\n",
        "\n",
        "| #channel for each layer \\ optimizer | SGD   | Adam  |\n",
        "|-------------------------------------|-------|-------|\n",
        "| (128, 128, 128)                     |       |       |\n",
        "| (256, 256, 256)                     |       |       |\n",
        "| (512, 512, 512)                     |       |       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go55LVSJd-vG"
      },
      "source": [
        "### 2) Full CNN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G0eCj6OmOEE"
      },
      "source": [
        "Based on the CNN in the previous question, implement a full CNN model with max pooling layer.\n",
        "\n",
        "- Add a max pooling layer after each convolutional layer.\n",
        "- Each max pooling layer has a kernel size of 2 and a stride of 2.\n",
        "\n",
        "Please implement this model in the following section. You will need to tune the hyperparameters and fill the results in the table. You are also required to complete the questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMrKGlMQhCa0"
      },
      "source": [
        "#### a) Implement max pooling layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2INt6P3Myd1"
      },
      "source": [
        "Copy the CNN implementation in previous question. Implement max pooling layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHu3Ic2dM1S9"
      },
      "outputs": [],
      "source": [
        "class CNN_MaxPool(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 128, kernel_size=3, padding=1),\n",
        "            nn.Linear(128 , 128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Linear(128 , 128),            \n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Linear(128 , 128),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(64*28*28 , 1024),\n",
        "            nn.Linear(1024, 10))\n",
        "    \n",
        "  def forward(self, images):\n",
        "    return self.network(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A6AEOoigq68"
      },
      "source": [
        "#### b) Tune hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drH4MHSVNqwz"
      },
      "source": [
        "Based on best optimizer you found in the previous problem, tune the number of channels and learning rate for best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT9Evw4hryj1"
      },
      "outputs": [],
      "source": [
        "# implement hyperparameters here\n",
        "model = skorch.NeuralNetClassifier(CNN_MaxPool,\n",
        "                                   criterion=torch.nn.CrossEntropyLoss,\n",
        "                                   device=\"cuda\")\n",
        "# implement input normalization & type cast here\n",
        "model.fit(train.data, train.targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Mu2ZZHoZU0"
      },
      "source": [
        "Write down the **validation accuracy** of your model under different hyperparameter settings.\n",
        "\n",
        "| #channel for each layer | validation accuracy |\n",
        "|-------------------------|---------------------|\n",
        "| (128, 128, 128)         |                     |\n",
        "| (128, 256, 512)         |                     |\n",
        "| (256, 256, 256)         |                     |\n",
        "| (256, 512, 1024)        |                     |\n",
        "| (512, 512, 512)         |                     |\n",
        "| (512, 1024, 2048)       |                     |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UCaz8nWoWWS"
      },
      "source": [
        "For the best model you have, test it on the test set.\n",
        "\n",
        "It is fine if you found some hyperparameter combination better than those listed in the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTtBk22OECDD"
      },
      "outputs": [],
      "source": [
        "# implement the same input normalization & type cast here\n",
        "test.predictions = model.predict(test.data)\n",
        "sklearn.metrics.accuracy_score(test.targets, test.predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSbhC8f1or6_"
      },
      "source": [
        "How much **test accuracy** do you get?\n",
        "\n",
        "**Your Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG3E9ZckomkX"
      },
      "source": [
        "What can you conclude for the design of CNN structure?\n",
        "\n",
        "**Your Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNlnDbZtqHgi"
      },
      "source": [
        "- **Subtask 2-1: Completing the Table**\n",
        "\n",
        "We have provided the following table for different combinations of optimizers and learning rate, please write down the **validation accuracy** of your model with different optimizers and learning rates.\n",
        "\n",
        "|         | 0.1  | 0.01 | 0.001|0.0001|\n",
        "|---------|------|------|------|------|\n",
        "| SGD     |      |      |      |      |\n",
        "| Adam    |      |      |      |      |\n",
        "| RMSprop |      |      |      |      |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVVd5b9rzSFA"
      },
      "source": [
        "- **Subtask 2-2: Explaining your Observations**\n",
        "\n",
        "Based on your results, briefly explain your observations, e.g., which optimizer works the best, what is the optimal learning rate for each optimizer?\n",
        "\n",
        "*Your Answer:*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyy7qpHtzYJn"
      },
      "source": [
        "### 3) Compare the Results under Different Epoches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhXQERRizZO6"
      },
      "source": [
        "In this task, we hope to compare the results of our model under different training epoches, and answer a question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JBBjTGd1zis"
      },
      "source": [
        "- **Subtask 3-1: Completing the Table**\n",
        "\n",
        "We have provided the following table, please write down the **training accuracy** and **validation accuracy** of your model under different epoches.\n",
        "\n",
        "|                    |  10  |  20  |  30  |  40  |  50  |\n",
        "|--------------------|------|------|------|------|------|\n",
        "| Training Accuracy  |      |      |      |      |      |\n",
        "| Validation Accuracy|      |      |      |      |      |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l82_FjXazdod"
      },
      "source": [
        "- **Subtask 3-2: Answering the Question**\n",
        "\n",
        "Is it always better to train a model for more epoches? How can we decide when should we stop training?\n",
        "\n",
        "*Your Answer:*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
